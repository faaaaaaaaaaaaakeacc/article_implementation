{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TMANet\n\nВ этом ноутбуке будет реализована сеть TMANet из этой статьи: https://arxiv.org/abs/2102.08643\n\n## Датасет:\n\nВ качестве датасета возьмем этот датасет: https://www.kaggle.com/datasets/carlolepelaars/camvid","metadata":{"id":"iW_SCLI1JG5n"}},{"cell_type":"code","source":"#from google.colab import files\n#files.upload()\n\n#!mkdir -p ~/.kaggle\n#!cp kaggle.json ~/.kaggle/\n#!pip install kaggle\n#!chmod 600 /root/.kaggle/kaggle.json\n#!kaggle datasets download -d carlolepelaars/camvid\n#!unzip camvid.zip","metadata":{"id":"PHItUnmSI5Rt","outputId":"0f8420f9-a72e-477c-fa23-19c698f2e1a6","execution":{"iopub.status.busy":"2022-06-25T20:26:41.153290Z","iopub.execute_input":"2022-06-25T20:26:41.153740Z","iopub.status.idle":"2022-06-25T20:26:41.178883Z","shell.execute_reply.started":"2022-06-25T20:26:41.153639Z","shell.execute_reply":"2022-06-25T20:26:41.177837Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим на значения классов: ","metadata":{"id":"lzF75Wx3Q4ZR"}},{"cell_type":"code","source":"import pandas as pd \n\ndf = pd.read_csv('../input/camvid/CamVid/class_dict.csv')\ndf","metadata":{"id":"sf2TxHHnO3BE","outputId":"ddc9f31b-8bb0-4057-a724-6269085150b8","execution":{"iopub.status.busy":"2022-06-25T20:26:42.283293Z","iopub.execute_input":"2022-06-25T20:26:42.284547Z","iopub.status.idle":"2022-06-25T20:26:42.316976Z","shell.execute_reply.started":"2022-06-25T20:26:42.284497Z","shell.execute_reply":"2022-06-25T20:26:42.316059Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Распарсим все изображения:","metadata":{"id":"mBURo7K2Syhq"}},{"cell_type":"code","source":"import torch\nfrom torchvision import transforms\nfrom PIL import Image\n\ndef get_tensor_image_from_path(path):\n    img = Image.open(path).resize((64, 64))\n    convert_tensor = transforms.ToTensor()\n    return convert_tensor(img)\n","metadata":{"id":"1wrOvcAiSeF5","execution":{"iopub.status.busy":"2022-06-25T20:26:42.542344Z","iopub.execute_input":"2022-06-25T20:26:42.543039Z","iopub.status.idle":"2022-06-25T20:26:44.506376Z","shell.execute_reply.started":"2022-06-25T20:26:42.542997Z","shell.execute_reply":"2022-06-25T20:26:44.505369Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import os\nname_video2images = dict()\nname_video2labels = dict()\n\nfor name_file in os.walk('../input/camvid/CamVid'):\n    if name_file[0] == '../input/camvid/CamVid/train':\n        for elem in name_file[2]:\n            if name_video2images.get(elem[:6], 0) == 0:\n                name_video2images[elem[:6]] = []\n            name_video2images[elem[:6]].append(elem)\n    if name_file[0] == '../input/camvid/CamVid/train_labels':\n         for elem in name_file[2]:\n            if name_video2labels.get(elem[:6], 0) == 0:\n                name_video2labels[elem[:6]] = []\n            name_video2labels[elem[:6]].append(elem)","metadata":{"id":"FmA3mZY-TgGx","execution":{"iopub.status.busy":"2022-06-25T20:26:44.509210Z","iopub.execute_input":"2022-06-25T20:26:44.510089Z","iopub.status.idle":"2022-06-25T20:26:45.046414Z","shell.execute_reply.started":"2022-06-25T20:26:44.510050Z","shell.execute_reply":"2022-06-25T20:26:45.045467Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"print(len(name_video2images))\nprint(len(name_video2labels))\nassert set(name_video2images.keys()) == set(name_video2labels.keys())","metadata":{"id":"ihzCwUn1VR6m","outputId":"29b58a60-f931-4ffe-f902-3af4bb1b0424","execution":{"iopub.status.busy":"2022-06-25T20:26:45.048072Z","iopub.execute_input":"2022-06-25T20:26:45.048741Z","iopub.status.idle":"2022-06-25T20:26:45.055517Z","shell.execute_reply.started":"2022-06-25T20:26:45.048703Z","shell.execute_reply":"2022-06-25T20:26:45.054385Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"for cluster in set(name_video2images.keys()):\n    print(len(name_video2images[cluster]),  len(name_video2labels[cluster]))\n    assert len(name_video2images[cluster]) == len(name_video2labels[cluster])","metadata":{"id":"2d_o6a65KnoL","outputId":"e14292a4-4ecd-4961-c3e7-83d3e93afc72","execution":{"iopub.status.busy":"2022-06-25T20:26:45.058950Z","iopub.execute_input":"2022-06-25T20:26:45.059640Z","iopub.status.idle":"2022-06-25T20:26:45.066266Z","shell.execute_reply.started":"2022-06-25T20:26:45.059576Z","shell.execute_reply":"2022-06-25T20:26:45.065133Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Посортируем в каждом видео данные:","metadata":{"id":"JT5-MulqWGzN"}},{"cell_type":"code","source":"for key in name_video2images.keys():\n    name_video2images[key] = sorted(name_video2images[key])\n    name_video2labels[key] = sorted(name_video2labels[key])","metadata":{"id":"2C9z-ZUFVS1o","outputId":"23b7f4b1-3976-4ad2-ad90-0bf545418205","execution":{"iopub.status.busy":"2022-06-25T20:26:45.067898Z","iopub.execute_input":"2022-06-25T20:26:45.068580Z","iopub.status.idle":"2022-06-25T20:26:45.074322Z","shell.execute_reply.started":"2022-06-25T20:26:45.068543Z","shell.execute_reply":"2022-06-25T20:26:45.073071Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Проверим у всех ли изображений одинаковый размер:","metadata":{"id":"tnsI7779XLxg"}},{"cell_type":"code","source":"H, W = -1, -1\nimage_dir = '../input/camvid/CamVid/train/'\nfor key in name_video2images.keys():\n    for path in name_video2images[key]:\n        cur_tensor = get_tensor_image_from_path(image_dir + path)\n        if H == -1:\n            H = int(cur_tensor.shape[1])\n            W = int(cur_tensor.shape[2])\n        assert H == int(cur_tensor.shape[1])\n        assert W == int(cur_tensor.shape[2])","metadata":{"id":"hq2iuHYSWDN6","execution":{"iopub.status.busy":"2022-06-25T20:26:45.076140Z","iopub.execute_input":"2022-06-25T20:26:45.076866Z","iopub.status.idle":"2022-06-25T20:27:03.735669Z","shell.execute_reply.started":"2022-06-25T20:26:45.076830Z","shell.execute_reply":"2022-06-25T20:27:03.734341Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Наконец, напишем класс для изображений:","metadata":{"id":"N0hkfyo3hUfW"}},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\n\nclass VideoDataset(Dataset):\n    def __init__(self, image_dict, labels_dict, image_dir, label_dir):\n        self.image_dict = image_dict\n        self.labels_dict = labels_dict\n        self.image_dir = image_dir\n        self.label_dir = label_dir\n\n    def __getitem__(self, index):\n        image_tensors = []\n        label_tensors = []\n        segment = list(self.image_dict.keys())[index]\n        for image_path in self.image_dict[segment]:\n            image_tensors.append(get_tensor_image_from_path(self.image_dir + image_path))\n        for image_path in self.labels_dict[segment]:\n            label_tensors.append(get_tensor_image_from_path(self.label_dir + image_path))\n        return image_tensors, label_tensors\n\n    def __len__(self):\n        return len(self.image_dict.keys())","metadata":{"id":"TFw9fB1weruc","execution":{"iopub.status.busy":"2022-06-25T20:27:03.738440Z","iopub.execute_input":"2022-06-25T20:27:03.738908Z","iopub.status.idle":"2022-06-25T20:27:03.750772Z","shell.execute_reply.started":"2022-06-25T20:27:03.738861Z","shell.execute_reply":"2022-06-25T20:27:03.749646Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_dataset = VideoDataset(name_video2images, name_video2labels, '../input/camvid/CamVid/train/', '../input/camvid/CamVid/train_labels/')","metadata":{"id":"1uhK-ySuk_Ua","execution":{"iopub.status.busy":"2022-06-25T20:27:03.752787Z","iopub.execute_input":"2022-06-25T20:27:03.753212Z","iopub.status.idle":"2022-06-25T20:27:03.759573Z","shell.execute_reply.started":"2022-06-25T20:27:03.753161Z","shell.execute_reply":"2022-06-25T20:27:03.758262Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Сделаем также датасет для валидации и теста:","metadata":{"id":"bTRI_60fmOaD"}},{"cell_type":"code","source":"name_video2images_valid = dict()\nname_video2labels_valid = dict()\n\nfor name_file in os.walk('../input/camvid/CamVid'):\n    if name_file[0] == '../input/camvid/CamVid/val':\n        for elem in name_file[2]:\n            if name_video2images_valid.get(elem[:6], 0) == 0:\n                name_video2images_valid[elem[:6]] = []\n            name_video2images_valid[elem[:6]].append(elem)\n    if name_file[0] == '../input/camvid/CamVid/val_labels':\n         for elem in name_file[2]:\n            if name_video2labels_valid.get(elem[:6], 0) == 0:\n                name_video2labels_valid[elem[:6]] = []\n            name_video2labels_valid[elem[:6]].append(elem)\n\nfor key in name_video2images_valid.keys():\n    name_video2images_valid[key] = sorted(name_video2images_valid[key])\n    name_video2labels_valid[key] = sorted(name_video2labels_valid[key])","metadata":{"id":"OUX8pHDpmT_9","execution":{"iopub.status.busy":"2022-06-25T20:27:03.761437Z","iopub.execute_input":"2022-06-25T20:27:03.762451Z","iopub.status.idle":"2022-06-25T20:27:03.779275Z","shell.execute_reply.started":"2022-06-25T20:27:03.762408Z","shell.execute_reply":"2022-06-25T20:27:03.778095Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"valid_dataset = VideoDataset(name_video2images_valid, name_video2labels_valid,'../input/camvid/CamVid/val/', '../input/camvid/CamVid/val_labels/')","metadata":{"id":"tqzZlER_oH-w","execution":{"iopub.status.busy":"2022-06-25T20:27:03.783844Z","iopub.execute_input":"2022-06-25T20:27:03.784125Z","iopub.status.idle":"2022-06-25T20:27:03.789177Z","shell.execute_reply.started":"2022-06-25T20:27:03.784100Z","shell.execute_reply":"2022-06-25T20:27:03.788013Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Датасет готов, попробуем какую-нибудь простую модель чтобы сравнивать результаты, например UNet:","metadata":{"id":"7kd1CJOclQJ3"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn \n\nclass Block(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n\n    def forward(self, X):\n        return self.model(X)\n","metadata":{"id":"OAB3ZmZolNdg","execution":{"iopub.status.busy":"2022-06-25T20:27:03.791077Z","iopub.execute_input":"2022-06-25T20:27:03.791694Z","iopub.status.idle":"2022-06-25T20:27:03.800902Z","shell.execute_reply.started":"2022-06-25T20:27:03.791653Z","shell.execute_reply":"2022-06-25T20:27:03.799712Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class UNet(nn.Module):\n    def __init__(self, in_channels = 3):\n        super().__init__()\n        self.conv1 = nn.Sequential(\n                  Block(in_channels, 16),\n                  Block(16, 16), \n                  Block(16, 16)\n        )\n        self.pool1 = nn.MaxPool2d(3, 1, 1, return_indices=True)\n        self.conv2 = nn.Sequential(\n                Block(16, 8),\n                Block(8, 8), \n                Block(8, 8)\n        )     \n        self.pool2 = nn.MaxPool2d(3, 1, 1, return_indices=True)\n        self.conv3 = Block(8, 8)\n\n        self.unpool1 = nn.MaxUnpool2d(3, 1, 1)\n\n        self.up1 = nn.Sequential(\n                Block(8, 8),\n                Block(8, 16)\n        )\n        self.unpool2 = nn.MaxUnpool2d(3, 1, 1)\n\n        self.up2 = nn.Sequential(\n                Block(16, 16),\n                Block(16, 3)\n        )\n        self.last = nn.Sequential(\n            Block(6, 9),\n            Block(9, 6),\n            nn.Conv2d(6, 3, 3, 1, 1)\n        )\n        self.sigmoid = nn.Sigmoid()\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, X):\n        hidden = self.conv1(X)\n        hidden, ind1 = self.pool1(hidden) \n        hidden = self.conv2(hidden)\n        hidden = self.dropout(hidden)\n        hidden, ind2 = self.pool2(hidden)\n        hidden = self.conv3(hidden) \n        hidden = self.dropout(hidden)\n        hidden = self.unpool1(hidden, ind2)\n        hidden = self.up1(hidden)\n        hidden = self.dropout(hidden)\n        hidden = self.unpool2(hidden, ind1)\n        hidden = self.up2(hidden)\n        hidden = torch.cat([hidden, X], 1)\n        return self.sigmoid(self.last(hidden))","metadata":{"id":"0SlxHU3Pnd2N","execution":{"iopub.status.busy":"2022-06-25T20:27:03.802748Z","iopub.execute_input":"2022-06-25T20:27:03.803134Z","iopub.status.idle":"2022-06-25T20:27:03.820386Z","shell.execute_reply.started":"2022-06-25T20:27:03.803096Z","shell.execute_reply":"2022-06-25T20:27:03.819252Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Обучим UNet и посмотрим результаты:","metadata":{"id":"cLKfDOHMp7Gh"}},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"id":"gkmQsh7oqqi7","outputId":"db3af177-d4a2-4a70-9f55-598e26e82b0f","execution":{"iopub.status.busy":"2022-06-25T20:27:03.822306Z","iopub.execute_input":"2022-06-25T20:27:03.823218Z","iopub.status.idle":"2022-06-25T20:27:03.883553Z","shell.execute_reply.started":"2022-06-25T20:27:03.823162Z","shell.execute_reply":"2022-06-25T20:27:03.882509Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"H, W","metadata":{"id":"9bbbzB33BtxE","outputId":"de23297f-ebf1-4822-db91-fb3d98d6aebf","execution":{"iopub.status.busy":"2022-06-25T20:27:03.885614Z","iopub.execute_input":"2022-06-25T20:27:03.886382Z","iopub.status.idle":"2022-06-25T20:27:03.897964Z","shell.execute_reply.started":"2022-06-25T20:27:03.886342Z","shell.execute_reply":"2022-06-25T20:27:03.896762Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model_baseline = UNet().to(device)\noptimizer = torch.optim.Adam(model_baseline.parameters(), lr = 1e-3)\ncriterion = nn.MSELoss()","metadata":{"id":"caJvEWV7pe3I","execution":{"iopub.status.busy":"2022-06-25T20:27:03.899718Z","iopub.execute_input":"2022-06-25T20:27:03.900541Z","iopub.status.idle":"2022-06-25T20:27:06.777951Z","shell.execute_reply.started":"2022-06-25T20:27:03.900505Z","shell.execute_reply":"2022-06-25T20:27:06.776973Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm\nnum_epochs = 40 \n\nfor epoch in tqdm(range(num_epochs)):\n    model_baseline.train()\n    sum_loss, cnt_loss = 0, 0\n    for list_videos, list_labels in train_dataset:\n        for i in range(len(list_videos)):\n            optimizer.zero_grad()\n            image = list_videos[i].unsqueeze(0).to(device)\n            label = list_labels[i].unsqueeze(0).to(device)\n            output = model_baseline(image)\n            loss = criterion(output, label)\n            loss.backward()\n            optimizer.step() \n            sum_loss += loss.item()\n            cnt_loss += 1\n    sum_val, cnt_val = 0, 0\n    with torch.no_grad():\n        for list_videos, list_labels in valid_dataset:\n            for i in range(len(list_videos)):\n                image = list_videos[i].unsqueeze(0).to(device)\n                label = list_labels[i].unsqueeze(0).to(device)\n                output = model_baseline(image)\n                loss = criterion(output, label)\n                sum_val += loss.item()\n                cnt_val += 1\n    print(f\"Mean train loss: {sum_loss / cnt_loss} | Mean valid loss: {sum_val / cnt_val}\")\n","metadata":{"id":"3vrQjnikr9EM","outputId":"d78dbe82-e077-43c7-8af7-45386f7ee528","execution":{"iopub.status.busy":"2022-06-25T20:27:06.779439Z","iopub.execute_input":"2022-06-25T20:27:06.779798Z","iopub.status.idle":"2022-06-25T20:28:01.884742Z","shell.execute_reply.started":"2022-06-25T20:27:06.779762Z","shell.execute_reply":"2022-06-25T20:28:01.881527Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Наконец, реализуем саму модель:","metadata":{"id":"DZvJkdv065OL"}},{"cell_type":"code","source":"class TMANet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = nn.Sequential(\n            Block(3, 4),\n            Block(4, 8),\n            Block(8, 10)\n        )\n        self.convA = nn.Sequential( # blue\n            Block(10, 10),\n            Block(10, 10)\n        )\n        self.convB = nn.Sequential( # green\n            Block(10, 10),\n            Block(10, 10)\n        )\n        self.previous_images = []\n        self.softmax = nn.Softmax()\n        self.prelast_net = nn.Sequential(\n            Block(20, 16),\n            Block(16, 8),\n            Block(8, 3)\n        )\n        self.last_net = nn.Sequential(\n            Block(6, 4),\n            Block(4, 3)\n        )\n        self.sigmoid = nn.Sigmoid()\n        self.lim_last = 6\n\n    def forward(self, X):\n        self.previous_images.append(X)\n        if len(self.previous_images) > self.lim_last:\n            popped_element = self.previous_images.pop(0)\n            del popped_element\n        memory_sequence = torch.cat(self.previous_images, 0)\n        backbone_output = self.backbone(memory_sequence)\n        hidden = self.convA(backbone_output) #T * cv * H * W\n        T = int(hidden.shape[0])\n        cv = int(hidden.shape[1])\n        H = int(hidden.shape[2])\n        W = int(hidden.shape[3])\n        MV = hidden.permute(0, 2, 3, 1).reshape(T * H * W, -1)\n        hidden_1 = self.convB(backbone_output) # T * ck * H * W   \n        ck = int(hidden_1.shape[1])\n        MK = hidden_1.permute(1, 0, 2, 3).reshape(ck, -1)\n        \n        backbone_cur_output = self.backbone(X)\n        hidden_cur = self.convA(backbone_cur_output)\n        QK = hidden_cur.permute(0, 2, 3, 1).reshape(H * W, -1)\n        hidden_cur_1 = self.convB(backbone_cur_output)\n        QV = hidden_cur_1\n\n        S = self.softmax(QK @ MK)\n        QK_add = S @ MV\n        QK_add = QK_add.permute(1, 0).reshape(1, cv, H, W)\n        QV = torch.cat((QV, QK_add), 1)\n        output = self.prelast_net(QV)\n        output = torch.cat([output, X], 1)\n        output = self.last_net(output)\n        output = self.sigmoid(output)\n        return output\n    def clear_history(self):\n        for elem in self.previous_images:\n            del elem","metadata":{"id":"lGvT91q72Sr1","execution":{"iopub.status.busy":"2022-06-25T20:28:06.095354Z","iopub.execute_input":"2022-06-25T20:28:06.095717Z","iopub.status.idle":"2022-06-25T20:28:06.112196Z","shell.execute_reply.started":"2022-06-25T20:28:06.095684Z","shell.execute_reply":"2022-06-25T20:28:06.111012Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"model = TMANet().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\ncriterion = nn.MSELoss()","metadata":{"id":"X_DZCCNV74nY","execution":{"iopub.status.busy":"2022-06-25T20:28:11.583365Z","iopub.execute_input":"2022-06-25T20:28:11.583735Z","iopub.status.idle":"2022-06-25T20:28:11.600548Z","shell.execute_reply.started":"2022-06-25T20:28:11.583704Z","shell.execute_reply":"2022-06-25T20:28:11.599426Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm\nnum_epochs = 30 \n\nfor epoch in tqdm(range(num_epochs)):\n    model.train()\n    sum_loss, cnt_loss = 0, 0\n    for list_videos, list_labels in train_dataset:\n        for i in range(len(list_videos)):\n            optimizer.zero_grad()\n            image = list_videos[i].unsqueeze(0).to(device)\n            label = list_labels[i].unsqueeze(0).to(device)\n            output = model(image)\n            loss = criterion(output, label)\n            loss.backward()\n            optimizer.step() \n            sum_loss += loss.item()\n            cnt_loss += 1\n        model.clear_history()\n    sum_val, cnt_val = 0, 0\n    with torch.no_grad():\n        for list_videos, list_labels in valid_dataset:\n            for i in range(len(list_videos)):\n                image = list_videos[i].unsqueeze(0).to(device)\n                label = list_labels[i].unsqueeze(0).to(device)\n                output = model(image)\n                loss = criterion(output, label)\n                sum_val += loss.item()\n                cnt_val += 1\n            model.clear_history()\n    print(f\"Mean train loss: {sum_loss / cnt_loss} | Mean valid loss: {sum_val / cnt_val}\")\n","metadata":{"id":"gChllY128N-z","outputId":"8995bda3-866f-450c-d792-61f1013f8a99","execution":{"iopub.status.busy":"2022-06-25T20:28:12.876182Z","iopub.execute_input":"2022-06-25T20:28:12.876832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Видно, что TMANet уступает обычному UNet, вероятно, нужно дольше обучать","metadata":{}},{"cell_type":"code","source":"","metadata":{"id":"tBFCKixi8gMz"},"execution_count":null,"outputs":[]}]}