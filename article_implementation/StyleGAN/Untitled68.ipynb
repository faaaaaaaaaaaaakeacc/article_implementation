{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled68.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lp55sX6XCkw"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!pip install kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d ashwingupta3012/human-faces\n",
        "!unzip human-faces\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "MZdpHBdTXa6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "import cv2\n",
        "\n",
        "\n",
        "pictures2show = ['/content/Humans/1 (1000).jpg', '/content/Humans/1 (1029).jpg',\n",
        "                 '/content/Humans/1 (1053).jpg', '/content/Humans/1 (1076).jpg',\n",
        "                 '/content/Humans/1 (1102).jpg', '/content/Humans/1 (1121).jpg',\n",
        "                 '/content/Humans/1 (1140).jpg', '/content/Humans/1 (1178).jpg', \n",
        "                 '/content/Humans/1 (1202).jpg', '/content/Humans/1 (1224).jpg']\n",
        "\n",
        "pic_box = plt.figure(figsize=(16,4))\n",
        " \n",
        "for i, picture in enumerate(pictures2show):\n",
        "    picture = cv2.imread(picture)\n",
        "    picture = cv2.cvtColor(picture, cv2.COLOR_BGR2RGB)\n",
        "    pic_box.add_subplot(2,5,i+1)\n",
        "    plt.imshow(picture)\n",
        "    plt.axis('off')\n",
        "plt.show()    "
      ],
      "metadata": {
        "id": "DBVl3k8yZYrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "def get_tensor_image_from_path(path):\n",
        "    img = Image.open(path).resize((256, 256))\n",
        "    convert_tensor = transforms.ToTensor()\n",
        "    convert_tensor.requires_grad=True\n",
        "    return convert_tensor(img)\n"
      ],
      "metadata": {
        "id": "_f-bR3FHg0AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "\n",
        "class FacesDataset(Dataset):\n",
        "    def __init__(self, image_dir = '/content/Humans/'):\n",
        "        self.images_pathes = []\n",
        "        for all_images in os.walk(image_dir):\n",
        "            for name_file in all_images[2]:\n",
        "                if name_file[-3:] == 'jpg' and get_tensor_image_from_path(image_dir + name_file).shape[0] == 3:\n",
        "                    self.images_pathes.append(image_dir + name_file)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return get_tensor_image_from_path(self.images_pathes[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_pathes)"
      ],
      "metadata": {
        "id": "vqRDviuXa8il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = FacesDataset()\n",
        "print(dataset[1].shape)\n",
        "plt.imshow(dataset[1].permute(1,2,0))"
      ],
      "metadata": {
        "id": "Krr88Fqgg8sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "id": "Y_aEWlmBhagf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_loader = DataLoader(dataset, batch_size=10, shuffle=True, num_workers=0)"
      ],
      "metadata": {
        "id": "xJiBkrY9hBV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in image_loader:\n",
        "    break"
      ],
      "metadata": {
        "id": "AwduBZTshdoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, 1, 1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Conv2d(32, 64, 4, 2, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Conv2d(64, 64, 4, 2, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Conv2d(64, 64, 3, 1, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Conv2d(64, 64, 3, 1, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Conv2d(64, 64, 3, 1, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Conv2d(64, 64, 3, 1, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Conv2d(64, 64, 4, 2, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Conv2d(64, 64, 4, 2, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Conv2d(64, 8, 4, 2, 1),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(8 * 8 * 8, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, input):\n",
        "        return self.model(input)\n",
        "    "
      ],
      "metadata": {
        "id": "oNEkP5Mqhe11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaIn(nn.Module):\n",
        "    def __init__(self, eps = 1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        mean_x = torch.mean(x, dim=[2,3])\n",
        "        mean_y = torch.mean(y, dim=[2,3])\n",
        "        std_x = torch.std(x, dim=[2,3])\n",
        "        std_y = torch.std(y, dim=[2,3])\n",
        "        mean_xu = mean_x.unsqueeze(-1).unsqueeze(-1)\n",
        "        mean_yu = mean_y.unsqueeze(-1).unsqueeze(-1)\n",
        "        std_xu = std_x.unsqueeze(-1).unsqueeze(-1) + self.eps\n",
        "        std_yu = std_y.unsqueeze(-1).unsqueeze(-1) + self.eps\n",
        "        return std_yu * ((x - mean_xu) / std_xu) + mean_yu"
      ],
      "metadata": {
        "id": "k5Zl08-MmiMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_sample = AdaIn()\n",
        "assert layer_sample(dataset[0].unsqueeze(0), dataset[1].unsqueeze(0)).shape == torch.Size([1, 3, 256, 256])"
      ],
      "metadata": {
        "id": "QjkOm_UQoExZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vec2Image(nn.Module):\n",
        "    def __init__(self, input_space = 512,\n",
        "                       out_channels = 3,\n",
        "                       image_size = 256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.first = nn.Sequential(\n",
        "            nn.Linear(input_space, out_channels * image_size * image_size)\n",
        "        )\n",
        "        self.input_space = input_space \n",
        "        self.out_channels = out_channels\n",
        "        self.image_size = image_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.shape[1] != self.input_space:\n",
        "            raise ValueError(\"Incorrect shape of input vector\")\n",
        "        out = self.first(x)\n",
        "        out = out.reshape(-1, self.out_channels, self.image_size, self.image_size)\n",
        "        return out"
      ],
      "metadata": {
        "id": "QAEyCLjNo7E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer = Vec2Image()\n",
        "\n",
        "input_vector = torch.rand(1, 512)\n",
        "output = layer(input_vector)\n",
        "assert output.shape == torch.Size([1, 3, 256, 256])"
      ],
      "metadata": {
        "id": "YVHNTP2bq5rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "id": "Yz47o-mvbNvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, \n",
        "                 in_channels, \n",
        "                 input_shape, \n",
        "                 out_channels,\n",
        "                 style_space=512\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.upsample = nn.Upsample(scale_factor = 2, mode='nearest')\n",
        "        self.layer_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.A_1 = Vec2Image(style_space, in_channels, 2 * input_shape)\n",
        "        self.layer_2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.A_2 = Vec2Image(style_space, out_channels, 2 * input_shape)\n",
        "        self.adain_1 = AdaIn()\n",
        "        self.adain_2 = AdaIn()\n",
        "\n",
        "    def forward(self, x, space_vector):\n",
        "        hiddenq = self.upsample(x)\n",
        "        hidden = self.layer_1(hiddenq)\n",
        "        noise = torch.rand(list(hidden.shape)).to(device) / 10\n",
        "        hiddenp = hidden + noise\n",
        "        style_image = self.A_1(space_vector)\n",
        "        hidden_1 = self.adain_1(hiddenp, style_image)\n",
        "        hidden_1p = self.layer_2(hidden_1)\n",
        "        noise1 = torch.rand(list(hidden_1p.shape)).to(device) / 10\n",
        "        hidden_1pq = hidden_1p + noise1\n",
        "        style_imagep = self.A_2(space_vector)\n",
        "        hidden_2 = self.adain_2(hidden_1pq, style_imagep)\n",
        "        return hidden_2"
      ],
      "metadata": {
        "id": "--o7m7bLtyUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer = Block(32, 32, 64).to(device)\n",
        "input_image = torch.rand(1, 32, 32, 32).to(device)\n",
        "vec = torch.rand(1, 512).to(device)\n",
        "print(layer(input_image, vec).shape)"
      ],
      "metadata": {
        "id": "gjnNH62o0gq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XdlH4jplbMQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StyleGAN(nn.Module):\n",
        "    def __init__(self, latent_space = 512, \n",
        "                       style_space = 512, \n",
        "                       ):\n",
        "        super().__init__()\n",
        "        self.latent_space = latent_space \n",
        "        self.style_space = style_space\n",
        "        self.fc_net = nn.Sequential(\n",
        "            nn.Linear( self.latent_space,  self.latent_space),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear( self.latent_space,  self.latent_space),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.latent_space, self.style_space),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.latent_space, self.style_space),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.latent_space, self.style_space)\n",
        "        )\n",
        "        \n",
        "        self.start_image = nn.Parameter(torch.rand(16, 32, 32).to(device))\n",
        "        self.block3 = Block(16, 32, 8, style_space)\n",
        "        self.block4 = Block(8, 64, 4, style_space)\n",
        "        self.block5 = Block(4, 128, 3, style_space)\n",
        "\n",
        "    def forward(self, x):\n",
        "        style_vec = self.fc_net(x)\n",
        "        batch_size = x.shape[0]\n",
        "        output = torch.cat([self.start_image.unsqueeze(0)] * batch_size, 0)        \n",
        "        output1 = self.block3(output, style_vec)\n",
        "        output2 = self.block4(output1, style_vec)\n",
        "        output3 = self.block5(output2, style_vec)\n",
        "        return output3\n",
        "\n"
      ],
      "metadata": {
        "id": "unw4jRLzjAxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "z9kvyXEUaxIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = StyleGAN().to(device)\n",
        "generated_image = model(torch.rand(1, 512).to(device))\n",
        "print(generated_image.shape)\n"
      ],
      "metadata": {
        "id": "PDFkvYGwzdkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(generated_image.squeeze(0).permute(1,2,0).cpu().detach().numpy())"
      ],
      "metadata": {
        "id": "43r-m8AI1QSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TDE5c12K9kGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = Discriminator().to(device)\n",
        "optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr = 1e-5)"
      ],
      "metadata": {
        "id": "L_JkOKqQ1yEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_generator = torch.optim.Adam(model.parameters(), lr = 1e-3)\n"
      ],
      "metadata": {
        "id": "gH_TyanA2VBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.autograd.set_detect_anomaly(True)"
      ],
      "metadata": {
        "id": "usoUvJ6NZ_n-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "num_pretrain = 100\n",
        "\n",
        "criterion_pretrain = nn.MSELoss()\n",
        "\n",
        "limit_unseen = 20\n",
        "cur = 0\n",
        "\n",
        "for epoch in tqdm(range(num_pretrain)):\n",
        "    for batch in tqdm(image_loader):\n",
        "        optimizer_generator.zero_grad()\n",
        "        input_vec = torch.rand((batch.shape[0], 512)).to(device)\n",
        "        fake_images = model(input_vec)\n",
        "        loss = criterion_pretrain(fake_images, batch.to(device))\n",
        "        loss.backward()\n",
        "        optimizer_generator.step()\n",
        "        plt.imshow(fake_images[0].squeeze(0).permute(1,2,0).cpu().detach().numpy())\n",
        "        plt.show()\n",
        "        break"
      ],
      "metadata": {
        "id": "sfemrBHhlAzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "num_epochs = 40\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "limit_unseen = 20\n",
        "cur = 0\n",
        "\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    for batch in tqdm(image_loader):\n",
        "        optimizer_discriminator.zero_grad()\n",
        "\n",
        "        output_real = discriminator(batch.to(device))\n",
        "        target_real = torch.ones(output_real.shape).to(device)\n",
        "        loss = criterion(output_real, target_real)\n",
        "\n",
        "        input_vec = torch.rand((batch.shape[0], 512)).to(device)\n",
        "        fake_images = model(input_vec)\n",
        "        output_fake = discriminator(fake_images)\n",
        "        target_fake = torch.zeros(output_fake.shape).to(device)\n",
        "        loss += criterion(output_fake, target_fake)\n",
        "        loss.backward()\n",
        "        optimizer_discriminator.step()\n",
        "\n",
        "        optimizer_generator.zero_grad()\n",
        "        fake_images = model(input_vec)\n",
        "        output_fake = discriminator(fake_images)\n",
        "        target_real = torch.zeros(output_fake.shape).to(device)\n",
        "        loss = criterion(output_fake, target_real)\n",
        "        loss.backward()\n",
        "        optimizer_generator.step()\n",
        "\n",
        "        cur += 1\n",
        "        if cur % limit_unseen == limit_unseen - 1:\n",
        "            plt.imshow(fake_images[0].squeeze(0).permute(1,2,0).cpu().detach().numpy())\n",
        "            plt.show()\n"
      ],
      "metadata": {
        "id": "9eCb26i-2D9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iQot3FIUXemW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}